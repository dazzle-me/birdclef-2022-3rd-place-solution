## TODO:
## augmentation idea:
## CONCAT-MIX: (same methodology as mixup i.e. take \
## random permutation then apply sample-wise)
## 1) concat two samples from batch (on time-axis)
## 2) label of concatenation is the union of labels
## 3) increase factor for this training step by 2

## NB: works only for the models that are allowed to take
## input of arbitrary length (e.g. model used in my solution)

general:
  description: "baseline + different frontend + tf_efficientnetv2_s_in21k \
  + mix-up + cutmix + spec-augment + background noise + pitch-shift"
  weights_path: null
  # teacher_path: /workspace/bclf/experiments/new-metrics-exp-17/fold_4/weights/epoch_28_metric_0.0703.pth
  # teacher_config:

  stage: 1
  device: cuda:0
  ## which fields from batch-dict to move to device
  fields_to_move: ['audio', 'target', 'weight']

  ## if using data from birdclef-2021 and birfclef-2022
  #scored_birds : [5, 18, 20, 31, 150, 165, 167, 215, 217, 218, 219, 220, 228, 233, 237, 277, 307, 337, 406, 454, 498]

  ## if using competition data (birdclef-2022) + wrong labels
  # scored_birds : [153, 156, 157, 9, 44, 185, 47, 194, 196, 197, 64, 65, 199, 201, 202, 90, 219, 111, 239, 246, 253]

  ## if using competition data (birdclef-2022) + treating all labels the same
  scored_birds: [3, 6, 7, 9, 44, 46, 47, 60, 62, 63, 64, 65, 67, 70, 72, 90, 101, 111, 131, 141, 150]
  scored_birds_names: ["akiapo", "aniani", "apapan", "barpet", "crehon", "elepai", "ercfra", "hawama", "hawcre", "hawgoo", "hawhaw", "hawpet1", "houfin", "iiwi", "jabwar", "maupar", "omao", "puaioh", "skylar", "warwhe1", "yefcan"]

  target_field: target

  exp_root: '/workspace/bclf/experiments/'
  exp_name: new-metrics-exp-19
  dev: False

logger:
  name: text_logger
  output_file: log.txt
  frequency: 25
  window_size: 500

data:
  use_uemu: False
  csv_file: train_folds.csv

  dir: /workspace/datasets/bird-clef/
  audio_dir: /workspace/datasets/bird-clef/train_audio/

  use_secondary_labels: True
  treat_secondary_unique: False
  secondary_label_value: 1.0

  label_smoothing: 0.0

  crop_length: 30
  val_crop_length: 30

  min_audio_length: 0
  max_audio_length: 9999999999999999

  sample_rate: 32000
  window_size: 2048
  hop_size: 512
  fmin: 0
  fmax: 16000
  mel_bins: 256
  power: 2

  # sample_rate: 32000
  # window_size: 1024
  # hop_size: 320
  # fmin: 50
  # fmax: 14000
  # mel_bins: 64
  # power: 2

  mel_norm: True

  batch_size: 16
  val_batch_size: 32
  num_workers: 14

  pin_memory_train: False
  pin_memory_val: False
  use_sampler: False

training:
  save_val: True
  epochs: 40
  k_fold: False

  num_folds: 5
  train_folds: [0, 1, 2, 3]
  val_folds: [4]

  optimizer:
    name: Adam
    lr: 0.0004
    wd: 0
    momentum: 0.9

  scheduler:
    name: CosineLRScheduler
    ReduceLROnPlateau:
      mode: max
      factor: 0.5
      patience: 3
      min_lr: 0.00001
      delta: 0.005
    CosineLRScheduler:
      cycle_length: 40
      cycle_decay: 0.17
      cycle_limit: 13337
      warmup_epochs: 0
      lr_min: 0.00001
      lr_warmup: 0.00005

  use_fp16: True

  train_loss: BCEWithLogitsLoss
  val_loss: BCEWithLogitsLoss
  # train_loss: FocalBCEWithLogits
  # val_loss: FocalBCEWithLogits

  scored_birds_scale: 1
  augment_non_scored: True

  clip_grad_norm: False
  augs:
    add_background_noise: True
    random_power:
      left: 2.0
      right: 2.0

    half_swap: 0.0
    time_reverse: 0.0

    mix_beta: 1.0

    mixup: 1.0
    cutmix: 1.0

    white_noise: 0.0
    concat_mix: 0.0

    gaussian_noise:
      proba: 0.0
      min_snr: 5
      max_snr: 40
    pitch_shift:
      proba: 0.5
      max_range: 5
    spec_augment:
      proba: 0.5
      time_drop_width: 64
      time_stripes_num: 2
      freq_drop_width: 8
      freq_stripes_num: 2

    time_stretch: 0.0
    time_stretch_bounds: [0.9, 1.1]

## b6 - 2304
## b5 - 2048
## b0 - 1280

model:
  pretrained: True
  name: Net
  # backbone: 'tf_efficientnetv2_m_in21k'
  # backbone: tf_efficientnet_b1_ns
  # backbone: swin_base_patch4_window12_384
  # backbone: tf_efficientnet_b0_ns
  # backbone: tf_efficientnet_b3_ns
  backbone: tf_efficientnetv2_s_in21k
  global_pool: gem
  num_classes: 152
  use_coord_conv: False
  in_chans: 1

loss:
  BCEWithLogitsLoss:
    input: 'logits_raw'
    output: 'target'
    reduction: 'none'
  FocalBCEWithLogits:
    input: 'logits_raw'
    output: 'target'
    reduction: 'none'
    gamma: 2.0
    alpha: 1.0

utils:
  save_handler:
    mode: min
    monitor: validation_loss
    top_1: True
    save_all: False

  early_stopping:
    mode: min
    patience: 99999

metrics:
  f1@0.1:
    input: logits
    output: target
    average: none
    threshold: 0.1
    name: f1@0.1
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.1_micro:
    input: logits
    output: target
    average: micro
    threshold: 0.1
    name: f1@0.1_micro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.1_macro:
    input: logits
    output: target
    average: macro
    threshold: 0.1
    name: f1@0.1_macro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']
####################

  f1@0.3:
    input: logits
    output: target
    average: none
    threshold: 0.3
    name: f1@0.3
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.3_micro:
    input: logits
    output: target
    average: micro
    threshold: 0.3
    name: f1@0.3_micro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.3_macro:
    input: logits
    output: target
    average: macro
    threshold: 0.3
    name: f1@0.3_macro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

###############
  f1@0.5:
    input: logits
    output: target
    average: none
    threshold: 0.5
    name: f1@0.5
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.5_micro:
    input: logits
    output: target
    average: micro
    threshold: 0.5
    name: f1@0.5_micro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']

  f1@0.5_macro:
    input: logits
    output: target
    average: macro
    threshold: 0.5
    name: f1@0.5_macro
    score_birds: ['jabwar', 'yefcan', 'skylar', 'akiapo', 'apapan', 'barpet', 'elepai', 'iiwi', 'houfin', 'omao', 'warwhe1', 'aniani', 'hawama', 'hawcre']
###################
  bird_metric:
    input: logits
    output: target
    threshold: 0.05
